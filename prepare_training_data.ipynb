{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate simulator and site images using Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_FOLDER = os.path.join('.', 'data')\n",
    "SIM_FOLDER = os.path.join(DATA_FOLDER, 'simulator')\n",
    "ADDITIONAL_IMAGES_FOLDER = os.path.join(SIM_FOLDER, 'additional')\n",
    "EXISTING_ANNOTATIONS_FILE = os.path.join(SIM_FOLDER, 'sim_data_annotations.yaml')\n",
    "FASTER_RCNN_GRAPH = os.path.join('.', 'model', 'faster-rcnn_frozen.pb')\n",
    "COCO_LABELS_FILE = os.path.join('.', 'labels', 'mscoco_label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "tl_labels = {\"Green\" : 1, \"Red\" : 2, \"Yellow\" : 3, \"off\" : 4}\n",
    "tl_coco_class = 10\n",
    "tl_detection_threshold = 0.80\n",
    "image_h = 600\n",
    "image_w = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the image files.\n",
    "sim_image_files = glob.glob(os.path.join(ADDITIONAL_IMAGES_FOLDER, '**','*.jpg'), recursive=True)\n",
    "# Read in existing annotations\n",
    "existing_annotations = yaml.load(open(EXISTING_ANNOTATIONS_FILE, 'r').read())\n",
    "# Get classes for all images\n",
    "sim_image_data = pd.DataFrame(sim_image_files, columns=['Path'])\n",
    "sim_image_data['Class'] = sim_image_data.apply(lambda df: df['Path'].split(os.path.sep)[-2], axis=1)\n",
    "# Boxes contain list of [ymin, xmin, ymax, xmax] boxes\n",
    "sim_image_data['Boxes'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Faster-RCNN\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(FASTER_RCNN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 755/755 [12:51<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run inference on images\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        for idx, row in tqdm(sim_image_data.iterrows(), total=len(sim_image_data)):\n",
    "            image = Image.open(row['Path'])\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "            # Actual detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "            classes = np.squeeze(classes).astype(np.int32)\n",
    "            boxes = np.squeeze(boxes)\n",
    "            scores = np.squeeze(scores)\n",
    "            # Check if detections include traffic lights\n",
    "            traffic_lights = []\n",
    "            for obj_class, score, box in zip(classes, scores, boxes):\n",
    "                if obj_class == tl_coco_class:\n",
    "                    if score is not None and score > tl_detection_threshold:\n",
    "                        traffic_lights.append(box)\n",
    "            sim_image_data.at[idx, 'Boxes'] = traffic_lights\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(existing_annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations = []\n",
    "#Extract and normalize boxes in existing annotations\n",
    "for sample in existing_annotations:\n",
    "    new_sample = {}\n",
    "    new_sample['filename'] = 'data/simulator/'+sample['filename']\n",
    "    new_sample['annotations'] = []\n",
    "    for annotation in sample['annotations']:\n",
    "        box = {}\n",
    "        box['xmin'] = annotation['xmin']/image_w\n",
    "        box['xmax'] = (annotation['xmin'] + annotation['x_width'])/image_w\n",
    "        box['ymin'] = annotation['ymin']/image_h\n",
    "        box['ymax'] = (annotation['ymin'] + annotation['y_height'])/image_h\n",
    "        box['class'] = annotation['class']\n",
    "        new_sample['annotations'].append(box)\n",
    "    new_annotations.append(new_sample)\n",
    "\n",
    "    #Grab all the annotations from the additional dataset\n",
    "for idx, row in sim_image_data.iterrows():\n",
    "    new_sample = {}\n",
    "    new_sample['filename'] = row['Path'][1:].replace('\\\\', '/')\n",
    "    new_sample['annotations'] = []\n",
    "    for annotation in row['Boxes']:\n",
    "        box = {}\n",
    "        box['xmin'] = annotation[1]\n",
    "        box['xmax'] = annotation[3]\n",
    "        box['ymin'] = annotation[0]\n",
    "        box['ymax'] = annotation[2]\n",
    "        box['class'] = row['Class']\n",
    "        new_sample['annotations'].append(box)\n",
    "    new_annotations.append(new_sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data\\\\simulator\\\\final_annotations.yml', 'w') as outfile:\n",
    "    yaml.dump(new_annotations, outfile, default_flow_style=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
